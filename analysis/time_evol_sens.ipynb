{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e682508",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from utils import experiment_logger, attack_utility\n",
    "from secalign_refactored import secalign, config\n",
    "import algorithms.losses_experimental as losses_experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdec36ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rel_path = \"../secalign_refactored/secalign_models/meta-llama/Meta-Llama-3-8B-Instruct_dpo_NaiveCompletion_2024-11-12-17-59-06-resized\"\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(0)\n",
    "\n",
    "load_model = True\n",
    "if load_model:\n",
    "    model, tokenizer, frontend_delimiters, _ = secalign.load_lora_model(model_rel_path, load_model=load_model, device_map=\"cpu\")\n",
    "\n",
    "    inst_delm = config.DELIMITERS[frontend_delimiters][0]\n",
    "    data_delm = config.DELIMITERS[frontend_delimiters][1]\n",
    "    resp_delm = config.DELIMITERS[frontend_delimiters][2]\n",
    "\n",
    "    prompt_template = config.PROMPT_FORMAT[frontend_delimiters]\n",
    "    model = model.eval()\n",
    "    model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "    model.generation_config.temperature = 0.0\n",
    "    model.generation_config.do_sample=False\n",
    "else:\n",
    "    model, tokenizer, frontend_delimiters, _ = None, None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7924b017",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_FOLDER_PATH = \"logs/univ_comp_run_6\"\n",
    "\n",
    "logger = experiment_logger.ExperimentLogger(LOG_FOLDER_PATH)\n",
    "\n",
    "input_tokenized_data_list = next(logger.query({\"variable_name\": \"input_tokenized_data_list\"}))\n",
    "\n",
    "best_tokens_dicts_by_num_examples = OrderedDict()\n",
    "\n",
    "for best_tokens_dicts, metadata_dict in logger.query_with_metadata({\"variable_name\": \"best_tokens_dicts_chunk\"}):\n",
    "    trace_id = metadata_dict[\"trace_id\"]\n",
    "    if trace_id not in best_tokens_dicts_by_num_examples:\n",
    "        best_tokens_dicts_by_num_examples[trace_id] = []\n",
    "    best_tokens_dicts_by_num_examples[trace_id].append(best_tokens_dicts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295b04eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "IDX_TO_CARE_FOR = 0 # i.e. the one with the largest dataset\n",
    "RELEVANT_BATCH_SIZES_TAKEN = [2, 4, 6, 8, 10]\n",
    "relevant_best_tokens_dicts = best_tokens_dicts_by_num_examples[IDX_TO_CARE_FOR]\n",
    "\n",
    "input_tokenized_dataset_stepwise = [attack_utility.update_all_tokens(best_token_dict, input_tokenized_data_list) for best_token_dict in relevant_best_tokens_dicts]\n",
    "\n",
    "SENS_FREQ = 20\n",
    "\n",
    "sensitivities_list = []\n",
    "for step_num, input_tokenized_dataset in enumerate(input_tokenized_dataset_stepwise):\n",
    "    if step_num % SENS_FREQ != 0:\n",
    "        continue\n",
    "\n",
    "    average_sens_at_step = losses_experimental.dataset_average_sensitivities(model, tokenizer, input_tokenized_dataset, None)\n",
    "    sensitivities_list.append(average_sens_at_step)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
