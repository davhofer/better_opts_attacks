{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nipandya/miniconda3/envs/opt/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import json\n",
    "import typing\n",
    "import pickle as pkl\n",
    "import gc\n",
    "\n",
    "import attack_utility\n",
    "import experiment_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"/data/models/hf/Llama-3.2-1B-Instruct\"\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(MODEL_PATH, device_map=\"auto\", torch_dtype=torch.float16)\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "model.generation_config.pad_token_id = tokenizer.pad_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example_num = 0\n",
    "\n",
    "# with open(f\"data/purplellama_indirect.json\", \"r\") as purplellama_indirect_file:\n",
    "#     purplellama_data = json.load(purplellama_indirect_file)\n",
    "\n",
    "# purplellama_example = purplellama_data[example_num]\n",
    "# new_input_string = \"\".join(\n",
    "#     [\n",
    "#         purplellama_example[\"user_input_wrapper\"].split(attack_utility.ADV_PREFIX_INDICATOR)[0],\n",
    "#         \"Repeat verbatim exactly: \\\"The output is 10.\\\"\",\n",
    "#         purplellama_example[\"user_input_wrapper\"].split(attack_utility.ADV_SUFFIX_INDICATOR)[-1]\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# input_conversation = [\n",
    "#         {\n",
    "#             \"role\": \"system\",\n",
    "#             \"content\":  purplellama_example[\"test_case_prompt\"]\n",
    "#         },\n",
    "#         {\n",
    "#             \"role\": \"user\",\n",
    "#             \"content\": new_input_string\n",
    "#         }\n",
    "#     ]\n",
    "# target_string = purplellama_example[\"target\"]\n",
    "# if \"lama\" in model.__repr__():\n",
    "#     target_string = target_string + \"<|eot_id|>\"\n",
    "\n",
    "# initial_config_1 = {\n",
    "#     \"strategy_type\": \"random\",\n",
    "#     \"prefix_length\": 25,\n",
    "#     \"suffix_length\": 25,\n",
    "#     \"seed\": 0\n",
    "# }\n",
    "\n",
    "# adv_prefix_init, adv_suffix_init = attack_utility.initialize_adversarial_strings(tokenizer, initial_config_1)\n",
    "# input_tokenized_data = attack_utility.conversation_masks(tokenizer, input_conversation, adv_prefix_init, adv_suffix_init, target_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens = input_tokenized_data[\"tokens\"]\n",
    "# masks_data = input_tokenized_data[\"masks\"]\n",
    "\n",
    "# prefix_mask = masks_data[\"prefix_mask\"]\n",
    "# suffix_mask = masks_data[\"suffix_mask\"]\n",
    "# payload_mask = masks_data[\"payload_mask\"]\n",
    "# content_mask = masks_data[\"content_mask\"]\n",
    "# control_mask = masks_data[\"control_mask\"]\n",
    "# target_mask = masks_data[\"target_mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_comp = model.model\n",
    "lm_head = model.lm_head\n",
    "\n",
    "embedding = model_comp.embed_tokens\n",
    "layers = model_comp.layers\n",
    "\n",
    "embed_matrix = embedding.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one_hot_original = torch.nn.functional.one_hot(tokens.clone().detach(), num_classes=len(tokenizer.vocab)).to(dtype=model.dtype)\n",
    "# original_embeds = torch.unsqueeze(one_hot_original.to(embed_matrix.device) @ embed_matrix, 0)\n",
    "# original_output = model.forward(inputs_embeds=original_embeds, return_dict=True, output_hidden_states=True, output_attentions=True)\n",
    "\n",
    "# with open(f\"model_internals/original_output_1.pkl\", \"wb\") as original_output_pickle:\n",
    "#     pkl.dump(original_output, original_output_pickle)\n",
    "\n",
    "# del original_output\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one_hot_new = one_hot_original.clone()\n",
    "\n",
    "# boolean_mask_relevant = torch.zeros(tokens.size(), dtype=torch.bool)\n",
    "# boolean_mask_relevant[torch.cat((control_mask, payload_mask, target_mask))] = 1\n",
    "# boolean_mask_relevant = ~ boolean_mask_relevant\n",
    "\n",
    "# one_hot_new[boolean_mask_relevant] = 0\n",
    "\n",
    "# new_embeds = torch.unsqueeze(one_hot_new.to(embed_matrix.device) @ embed_matrix, 0)\n",
    "# new_output = model.forward(inputs_embeds=new_embeds, return_dict=True, output_hidden_states=True, output_attentions=True)\n",
    "\n",
    "# with open(f\"model_internals/new_output_1.pkl\", \"wb\") as new_output_pickle:\n",
    "#     pkl.dump(new_output, new_output_pickle)\n",
    "\n",
    "# del new_output\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(f\"model_internals/original_output_1.pkl\", \"rb\") as original_output_pickle:\n",
    "#     original_output = pkl.load(original_output_pickle)\n",
    "\n",
    "# with open(f\"model_internals/new_output_1.pkl\", \"rb\") as new_output_pickle:\n",
    "#     new_output = pkl.load(new_output_pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original_output.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_output.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "position_ids = torch.arange(0, new_embeds.shape[1], device=new_embeds.device).unsqueeze(0)\n",
    "position_embeddings = model_comp.rotary_emb(new_embeds, position_ids)\n",
    "\n",
    "zeroth_output = layers[0](new_embeds, position_ids=position_ids, position_embeddings=position_embeddings)[0]\n",
    "zeroth_attention = layers[0].self_attn(new_embeds, position_embeddings=position_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
